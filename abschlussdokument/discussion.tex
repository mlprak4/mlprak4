%!TEX root = bare_conf.tex

\section{Discussion}\label{sec:discussion}

In \ref{sec:evaluation} we illustrated how an intuitive distance based approach could be improved upon by adding a direct feedback component. The results were gradually improved with every action reward rule, but adding even more rules would not yield the same gains. We identified the current action set as the most limiting factor for further improvements. For better generalization a new training map should be selected, offering a greater variety of experiences.

The main conclusion we can draw from this project, is that the selection of the reward function is of utmost importance. Changes on different scales should be easy to implement, which is why sticking to a simple set of rules is more desirable than obtaining one convoluted expression. The action set is another very important aspect. While the set should be kept at a manageable size it should also adapt to the training goals. When adding new actions it makes a big difference whether these new actions are similar to the existing ones, or whether they add another layer of complexity to the problem. Since our action set is the limiting factor for the performance we did some tests with another approach using steering deltas instead of fixed steering angles. The net receives the current steering angle as additional input and has the option to slightly de- or increase this angle. On top of this we added control over the velocity using the same principle. In the few tests we did there was no comparable success, the car understood it's task, but could not yet handle the action set. This approach was obviously more complex, but also looked very promising. In terms of improving our current results, this would definitely be the approach of choice for us. The action set can still be kept small, but the new actions would allow for much smoother driving on any given track.

One of the most interesting and challenging features, we would have liked to add, is dealing with obstacles. This would require some new reward function rules to be added and most likely additional sensors to be used as input. Having a car interact with it's environment would be a big step towards real driving situations.

However we deem the transfer of the obtained results to a real self-driving car to be very difficult. The simplicity of the simulation allowed us to train a pretty small network on a very small input, real world cases would require preprocessing to recreate a similar level of simplicity. Otherwise the net would have to be a lot bigger to handle the complex input, which would most likely result in unfeasible training durations. Considering the action set it would be wise to switch to the above explained delta approach or something similar, because fixed steering angles can not be sensibly implemented for a real car and would need further computation.

%-obstacles
%-deltas
%-real world

\iffalse
From this project we can draw some conclusions. Firstly, selection of reward functions is very important. With a bad reward function the agent could learn nothing. Secondly, the training can also be successful even with a small net and small image. The deep neural networks we used is much smaller than the ones such as AlexNet or VGG. Finally, improper actions can lead to bad performance. For example, when we set the car a large speed, it can not follow the lanes any more.

However, it is difficult to transfer the success of this project to an application for real self-driving cars. In order to get more realistic results, some further experiments are needed be carried out. For instance, the car can be trained on more complex routes with traffic, crossing or obstacles. Furthermore, the steering angle in the actions must not remain fixed. They could have a ``delta'' increment. So the actions for turning could be ``more'' or ``less'' left and ``more'' or ``less'' right. We could take current steering angle along with the image as input of DQN. With this the car should run more smoothly at curves.
\fi